{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1c06b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "922f785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d8c6697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>cap-shape</th>\n",
       "      <th>cap-surface</th>\n",
       "      <th>cap-color</th>\n",
       "      <th>bruises</th>\n",
       "      <th>odor</th>\n",
       "      <th>gill-attachment</th>\n",
       "      <th>gill-spacing</th>\n",
       "      <th>gill-size</th>\n",
       "      <th>gill-color</th>\n",
       "      <th>...</th>\n",
       "      <th>stalk-surface-below-ring</th>\n",
       "      <th>stalk-color-above-ring</th>\n",
       "      <th>stalk-color-below-ring</th>\n",
       "      <th>veil-type</th>\n",
       "      <th>veil-color</th>\n",
       "      <th>ring-number</th>\n",
       "      <th>ring-type</th>\n",
       "      <th>spore-print-color</th>\n",
       "      <th>population</th>\n",
       "      <th>habitat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p</td>\n",
       "      <td>x</td>\n",
       "      <td>s</td>\n",
       "      <td>n</td>\n",
       "      <td>t</td>\n",
       "      <td>p</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>n</td>\n",
       "      <td>k</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>k</td>\n",
       "      <td>s</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e</td>\n",
       "      <td>x</td>\n",
       "      <td>s</td>\n",
       "      <td>y</td>\n",
       "      <td>t</td>\n",
       "      <td>a</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>b</td>\n",
       "      <td>k</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e</td>\n",
       "      <td>b</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>t</td>\n",
       "      <td>l</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>b</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p</td>\n",
       "      <td>x</td>\n",
       "      <td>y</td>\n",
       "      <td>w</td>\n",
       "      <td>t</td>\n",
       "      <td>p</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>k</td>\n",
       "      <td>s</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e</td>\n",
       "      <td>x</td>\n",
       "      <td>s</td>\n",
       "      <td>g</td>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>w</td>\n",
       "      <td>b</td>\n",
       "      <td>k</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>a</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8119</th>\n",
       "      <td>e</td>\n",
       "      <td>k</td>\n",
       "      <td>s</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>b</td>\n",
       "      <td>y</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>b</td>\n",
       "      <td>c</td>\n",
       "      <td>l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8120</th>\n",
       "      <td>e</td>\n",
       "      <td>x</td>\n",
       "      <td>s</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>b</td>\n",
       "      <td>y</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>n</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>b</td>\n",
       "      <td>v</td>\n",
       "      <td>l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8121</th>\n",
       "      <td>e</td>\n",
       "      <td>f</td>\n",
       "      <td>s</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>b</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>b</td>\n",
       "      <td>c</td>\n",
       "      <td>l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8122</th>\n",
       "      <td>p</td>\n",
       "      <td>k</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>y</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>n</td>\n",
       "      <td>b</td>\n",
       "      <td>...</td>\n",
       "      <td>k</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>e</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8123</th>\n",
       "      <td>e</td>\n",
       "      <td>x</td>\n",
       "      <td>s</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>b</td>\n",
       "      <td>y</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>o</td>\n",
       "      <td>c</td>\n",
       "      <td>l</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8124 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     class cap-shape cap-surface cap-color bruises odor gill-attachment  \\\n",
       "0        p         x           s         n       t    p               f   \n",
       "1        e         x           s         y       t    a               f   \n",
       "2        e         b           s         w       t    l               f   \n",
       "3        p         x           y         w       t    p               f   \n",
       "4        e         x           s         g       f    n               f   \n",
       "...    ...       ...         ...       ...     ...  ...             ...   \n",
       "8119     e         k           s         n       f    n               a   \n",
       "8120     e         x           s         n       f    n               a   \n",
       "8121     e         f           s         n       f    n               a   \n",
       "8122     p         k           y         n       f    y               f   \n",
       "8123     e         x           s         n       f    n               a   \n",
       "\n",
       "     gill-spacing gill-size gill-color  ... stalk-surface-below-ring  \\\n",
       "0               c         n          k  ...                        s   \n",
       "1               c         b          k  ...                        s   \n",
       "2               c         b          n  ...                        s   \n",
       "3               c         n          n  ...                        s   \n",
       "4               w         b          k  ...                        s   \n",
       "...           ...       ...        ...  ...                      ...   \n",
       "8119            c         b          y  ...                        s   \n",
       "8120            c         b          y  ...                        s   \n",
       "8121            c         b          n  ...                        s   \n",
       "8122            c         n          b  ...                        k   \n",
       "8123            c         b          y  ...                        s   \n",
       "\n",
       "     stalk-color-above-ring stalk-color-below-ring veil-type veil-color  \\\n",
       "0                         w                      w         p          w   \n",
       "1                         w                      w         p          w   \n",
       "2                         w                      w         p          w   \n",
       "3                         w                      w         p          w   \n",
       "4                         w                      w         p          w   \n",
       "...                     ...                    ...       ...        ...   \n",
       "8119                      o                      o         p          o   \n",
       "8120                      o                      o         p          n   \n",
       "8121                      o                      o         p          o   \n",
       "8122                      w                      w         p          w   \n",
       "8123                      o                      o         p          o   \n",
       "\n",
       "     ring-number ring-type spore-print-color population habitat  \n",
       "0              o         p                 k          s       u  \n",
       "1              o         p                 n          n       g  \n",
       "2              o         p                 n          n       m  \n",
       "3              o         p                 k          s       u  \n",
       "4              o         e                 n          a       g  \n",
       "...          ...       ...               ...        ...     ...  \n",
       "8119           o         p                 b          c       l  \n",
       "8120           o         p                 b          v       l  \n",
       "8121           o         p                 b          c       l  \n",
       "8122           o         e                 w          v       l  \n",
       "8123           o         p                 o          c       l  \n",
       "\n",
       "[8124 rows x 23 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('mushroom_dataset.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bf2190b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>cap-shape</th>\n",
       "      <th>cap-surface</th>\n",
       "      <th>cap-color</th>\n",
       "      <th>bruises</th>\n",
       "      <th>odor</th>\n",
       "      <th>gill-attachment</th>\n",
       "      <th>gill-spacing</th>\n",
       "      <th>gill-size</th>\n",
       "      <th>gill-color</th>\n",
       "      <th>...</th>\n",
       "      <th>stalk-surface-below-ring</th>\n",
       "      <th>stalk-color-above-ring</th>\n",
       "      <th>stalk-color-below-ring</th>\n",
       "      <th>veil-type</th>\n",
       "      <th>veil-color</th>\n",
       "      <th>ring-number</th>\n",
       "      <th>ring-type</th>\n",
       "      <th>spore-print-color</th>\n",
       "      <th>population</th>\n",
       "      <th>habitat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8119</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8120</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8121</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8122</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8123</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8124 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      class  cap-shape  cap-surface  cap-color  bruises  odor  \\\n",
       "0         1          5            2          4        1     6   \n",
       "1         0          5            2          9        1     0   \n",
       "2         0          0            2          8        1     3   \n",
       "3         1          5            3          8        1     6   \n",
       "4         0          5            2          3        0     5   \n",
       "...     ...        ...          ...        ...      ...   ...   \n",
       "8119      0          3            2          4        0     5   \n",
       "8120      0          5            2          4        0     5   \n",
       "8121      0          2            2          4        0     5   \n",
       "8122      1          3            3          4        0     8   \n",
       "8123      0          5            2          4        0     5   \n",
       "\n",
       "      gill-attachment  gill-spacing  gill-size  gill-color  ...  \\\n",
       "0                   1             0          1           4  ...   \n",
       "1                   1             0          0           4  ...   \n",
       "2                   1             0          0           5  ...   \n",
       "3                   1             0          1           5  ...   \n",
       "4                   1             1          0           4  ...   \n",
       "...               ...           ...        ...         ...  ...   \n",
       "8119                0             0          0          11  ...   \n",
       "8120                0             0          0          11  ...   \n",
       "8121                0             0          0           5  ...   \n",
       "8122                1             0          1           0  ...   \n",
       "8123                0             0          0          11  ...   \n",
       "\n",
       "      stalk-surface-below-ring  stalk-color-above-ring  \\\n",
       "0                            2                       7   \n",
       "1                            2                       7   \n",
       "2                            2                       7   \n",
       "3                            2                       7   \n",
       "4                            2                       7   \n",
       "...                        ...                     ...   \n",
       "8119                         2                       5   \n",
       "8120                         2                       5   \n",
       "8121                         2                       5   \n",
       "8122                         1                       7   \n",
       "8123                         2                       5   \n",
       "\n",
       "      stalk-color-below-ring  veil-type  veil-color  ring-number  ring-type  \\\n",
       "0                          7          0           2            1          4   \n",
       "1                          7          0           2            1          4   \n",
       "2                          7          0           2            1          4   \n",
       "3                          7          0           2            1          4   \n",
       "4                          7          0           2            1          0   \n",
       "...                      ...        ...         ...          ...        ...   \n",
       "8119                       5          0           1            1          4   \n",
       "8120                       5          0           0            1          4   \n",
       "8121                       5          0           1            1          4   \n",
       "8122                       7          0           2            1          0   \n",
       "8123                       5          0           1            1          4   \n",
       "\n",
       "      spore-print-color  population  habitat  \n",
       "0                     2           3        5  \n",
       "1                     3           2        1  \n",
       "2                     3           2        3  \n",
       "3                     2           3        5  \n",
       "4                     3           0        1  \n",
       "...                 ...         ...      ...  \n",
       "8119                  0           1        2  \n",
       "8120                  0           4        2  \n",
       "8121                  0           1        2  \n",
       "8122                  7           4        2  \n",
       "8123                  4           1        2  \n",
       "\n",
       "[8124 rows x 23 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## turning into label encoded vals\n",
    "le = LabelEncoder()\n",
    "for col in data.columns:\n",
    "    le.fit(data[col])\n",
    "    data[col] = le.transform(data[col])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c1523b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting data into train test\n",
    "X = data.iloc[:,1:]\n",
    "y = data['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.01, random_state=42) #only use k-fold so\n",
    "X_train = X_train.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_test = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd9c4c9",
   "metadata": {},
   "source": [
    "## 1. [10 pts] Report 10-fold CV performances of GaussianNB, linear SVC (use SVC(kernel='linear', probability=True)), MLPClassifier, and DecisionTreeClassifier with default parameters. Now report the RandomForestClassifier performance too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a902352",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a class to make it easier to iterate through multple classifiers\n",
    "from sklearn.base import BaseEstimator\n",
    "class ClfPipe(BaseEstimator):\n",
    "\n",
    "    def __init__(self, estimator, X_train, y_train, X_test, y_test, name):\n",
    "        self.estimator = estimator\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.name = name\n",
    "\n",
    "    def predict(self):\n",
    "        self.estimator.fit(self.X_train, self.y_train)\n",
    "        self.predictions = self.estimator.predict(self.X_test)\n",
    "    \n",
    "    def metrics(self):\n",
    "        self.predict()\n",
    "        metr = {'Name': self.name, 'Accuracy': accuracy_score(self.y_test, self.predictions), \n",
    "                       'F1': f1_score(self.y_test, self.predictions, average='weighted') }\n",
    "        return metr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcc3888d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DT</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GNB</th>\n",
       "      <td>0.920417</td>\n",
       "      <td>0.920417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.976623</td>\n",
       "      <td>0.976613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Accuracy        F1\n",
       "Name                    \n",
       "DT    1.000000  1.000000\n",
       "GNB   0.920417  0.920417\n",
       "MLP   1.000000  1.000000\n",
       "RF    1.000000  1.000000\n",
       "SVC   0.976623  0.976613"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#go through each of the classifiers in different splits\n",
    "scores = pd.DataFrame()\n",
    "kf = KFold(n_splits=10)\n",
    "kf.get_n_splits(X_train)\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]\n",
    "    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n",
    "    GNB = ClfPipe(GaussianNB(), X_train_fold, y_train_fold, X_test_fold, y_test_fold, 'GNB')\n",
    "    scores = scores.append(GNB.metrics(), ignore_index=True)\n",
    "    SV = ClfPipe(SVC(kernel='linear', probability = True), X_train_fold, y_train_fold, X_test_fold, y_test_fold, 'SVC')\n",
    "    scores = scores.append(SV.metrics(), ignore_index=True)\n",
    "    MLP = ClfPipe(MLPClassifier(), X_train_fold, y_train_fold, X_test_fold, y_test_fold, 'MLP')\n",
    "    scores = scores.append(MLP.metrics(), ignore_index=True)\n",
    "    DT = ClfPipe(DecisionTreeClassifier(), X_train_fold, y_train_fold, X_test_fold, y_test_fold, 'DT')\n",
    "    scores = scores.append(DT.metrics(), ignore_index=True)\n",
    "    RF = ClfPipe(RandomForestClassifier(), X_train_fold, y_train_fold, X_test_fold, y_test_fold, 'RF')\n",
    "    scores = scores.append(RF.metrics(), ignore_index=True)\n",
    "scores.groupby('Name').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa889495",
   "metadata": {},
   "source": [
    "## 2. [10 pts] Generate an ensemble of 100 classifiers for each of the four classifier in Q1. stored as a list. Set the neural network hidden sizes to (3, 3), max iterations to 30, and tolerance to 1e-1. Set the decision tree parameters to max depth of 5 and max features of 5. We will evaluate these four ensemble classifiers. For each of the ensemble, report the first classifier performance in the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b19bb37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a class to make it easier to iterate through multple classifiers, identified by strings\n",
    "from sklearn.base import BaseEstimator\n",
    "class EnsPipe(BaseEstimator):\n",
    "\n",
    "    def __init__(self, estimator, X_train, y_train):\n",
    "        if(estimator == 'gaus'):\n",
    "            self.estimator = GaussianNB()\n",
    "        elif(estimator == 'mlp'):\n",
    "            self.estimator = MLPClassifier(hidden_layer_sizes=(3,3), max_iter=30, tol=.1)\n",
    "        elif(estimator == 'svc'): \n",
    "            self.estimator =SVC(kernel='linear', probability = True, max_iter=30, tol=.1)\n",
    "        else:\n",
    "            self.estimator=DecisionTreeClassifier(max_depth=5, max_features=5)\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.name = estimator\n",
    "    \n",
    "    def fit(self):\n",
    "        self.estimator.fit(self.X_train, self.y_train)\n",
    "        return self.estimator\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        self.predictions = self.estimator.predict(self.X_test)\n",
    "        return self.predictions\n",
    "    \n",
    "    def metrics(self, X_test, y_test):\n",
    "        self.fit()\n",
    "        self.predict(X_test)\n",
    "        metr = {'Name': self.name, 'Accuracy': accuracy_score(self.y_test, self.predictions), \n",
    "                       'F1': f1_score(self.y_test, self.predictions, average='weighted')}\n",
    "        return metr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f06aec45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enseSample(X_train, n):\n",
    "    classifiers = ['gaus', 'svc', 'mlp', 'dt']\n",
    "    ensembles = []\n",
    "    for clas in classifiers:\n",
    "        classList = []\n",
    "        for index in range(0,100):\n",
    "            weak = {}\n",
    "            weak['indices'] = np.random.choice(X_train.shape[0], size=round(X_train.shape[0]*n), replace=False)\n",
    "            weak['class'] = clas\n",
    "            classList.append(weak)\n",
    "        ensembles.append(classList)\n",
    "    return ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3c873b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Name': 'gaus', 'Accuracy': 0.9146341463414634, 'F1': 0.9145704309707674}\n",
      "{'Name': 'svc', 'Accuracy': 0.5121951219512195, 'F1': 0.5017336202773792}\n",
      "{'Name': 'mlp', 'Accuracy': 0.5609756097560976, 'F1': 0.4678002048996171}\n",
      "{'Name': 'dt', 'Accuracy': 0.9634146341463414, 'F1': 0.9634309596108008}\n"
     ]
    }
   ],
   "source": [
    "initial =enseSample(X_train, .1)\n",
    "for clas in initial:\n",
    "    print(EnsPipe(clas[0]['class'], X_train[clas[0]['indices']], y_train[clas[0]['indices']]).metrics( X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb2b518",
   "metadata": {},
   "source": [
    "## 3. [20 pts] Write a function ensemble_fit() to receive the ensemble (i.e. one of the lists in Q2.) and train on one of the subsets of the training data (e.g. random.sample can generate a subset). So each classifier will see only a different subset of the training dataset, also called as subsampling the input data for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7944b26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_fit(ensembles, X_train, y_train):\n",
    "    ensObjects = []\n",
    "    for clas in ensembles:\n",
    "        classObjects = []\n",
    "        for index in range(len(clas)):\n",
    "            classObjects.append(EnsPipe(clas[index]['class'], X_train[clas[index]['indices']], y_train[clas[index]['indices']]).fit())\n",
    "        ensObjects.append(classObjects)\n",
    "    return ensObjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52db893f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB(),\n",
       "  GaussianNB()],\n",
       " [SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1),\n",
       "  SVC(kernel='linear', max_iter=30, probability=True, tol=0.1)],\n",
       " [MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1),\n",
       "  MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=30, tol=0.1)],\n",
       " [DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5),\n",
       "  DecisionTreeClassifier(max_depth=5, max_features=5)]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted = ensemble_fit(initial, X_train, y_train)\n",
    "fitted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b147ec0",
   "metadata": {},
   "source": [
    "## 4. [20 pts] Write a function ensemble_predict() to receive the trained ensemble (i.e. one of the lists in Q3.) and test on the input. Use a voting scheme such as a histogram on the returned predictions by c.predict() by each of the weak classifier. The final prediction should be the np.argmax() of those counts. (Note that c.predict_proba() should have better results.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "951d9ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#voting will all have the same weight\n",
    "def ensemble_predict(ensemble, X_test):\n",
    "    ensPred = []\n",
    "    for clas in ensemble:\n",
    "        predictions = pd.DataFrame()\n",
    "        for obj in range(len(clas)):\n",
    "            predictions[obj] =clas[obj].predict(X_test)\n",
    "        ensPred.append(round(predictions.mean(axis=1)).astype(int))\n",
    "    return ensPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3822f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0     0\n",
       " 1     1\n",
       " 2     1\n",
       " 3     1\n",
       " 4     1\n",
       "      ..\n",
       " 77    1\n",
       " 78    1\n",
       " 79    1\n",
       " 80    0\n",
       " 81    1\n",
       " Length: 82, dtype: int64,\n",
       " 0     0\n",
       " 1     0\n",
       " 2     0\n",
       " 3     1\n",
       " 4     0\n",
       "      ..\n",
       " 77    0\n",
       " 78    0\n",
       " 79    1\n",
       " 80    0\n",
       " 81    0\n",
       " Length: 82, dtype: int64,\n",
       " 0     0\n",
       " 1     1\n",
       " 2     1\n",
       " 3     0\n",
       " 4     1\n",
       "      ..\n",
       " 77    1\n",
       " 78    0\n",
       " 79    0\n",
       " 80    0\n",
       " 81    1\n",
       " Length: 82, dtype: int64,\n",
       " 0     0\n",
       " 1     1\n",
       " 2     1\n",
       " 3     0\n",
       " 4     1\n",
       "      ..\n",
       " 77    1\n",
       " 78    1\n",
       " 79    1\n",
       " 80    0\n",
       " 81    1\n",
       " Length: 82, dtype: int64]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred =ensemble_predict(fitted, X_test)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1fd931",
   "metadata": {},
   "source": [
    "## 5. [20 pts] Report 10-fold CV performances of the ensembles with a subsample ratio of 0.1. Compare to a regular decision tree (same subsample ratio). Now repeat these for subsample of 0.001.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d70729",
   "metadata": {},
   "source": [
    "At some point, the subsamples are so small that there is only one class sampled. For 5 and six, I will be using subsamples of .1, .05, .01, .005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3f16ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of .1 regular Decision Tree {'Name': 'dt', 'Accuracy': 0.9950310559006211, 'F1': 0.9950328129747905}\n",
      "Sample of .01 regular Decision Tree {'Name': 'dt', 'Accuracy': 0.9763975155279503, 'F1': 0.9763952438775332}\n",
      "Sample of .1 regular Decision Tree {'Name': 'dt', 'Accuracy': 0.9875776397515528, 'F1': 0.987577716429949}\n",
      "Sample of .01 regular Decision Tree {'Name': 'dt', 'Accuracy': 0.9540372670807453, 'F1': 0.9540183946823864}\n",
      "Sample of .1 regular Decision Tree {'Name': 'dt', 'Accuracy': 1.0, 'F1': 1.0}\n",
      "Sample of .01 regular Decision Tree {'Name': 'dt', 'Accuracy': 0.9427860696517413, 'F1': 0.9426816977082902}\n",
      "Sample of .1 regular Decision Tree {'Name': 'dt', 'Accuracy': 0.9937810945273632, 'F1': 0.9937792140870324}\n",
      "Sample of .01 regular Decision Tree {'Name': 'dt', 'Accuracy': 0.8644278606965174, 'F1': 0.8642165456470577}\n",
      "Sample of .1 regular Decision Tree {'Name': 'dt', 'Accuracy': 1.0, 'F1': 1.0}\n",
      "Sample of .01 regular Decision Tree {'Name': 'dt', 'Accuracy': 0.9340796019900498, 'F1': 0.9340792960509217}\n",
      "Sample of .1 regular Decision Tree {'Name': 'dt', 'Accuracy': 0.9925373134328358, 'F1': 0.9925374519770959}\n",
      "Sample of .01 regular Decision Tree {'Name': 'dt', 'Accuracy': 0.9303482587064676, 'F1': 0.9302977786139763}\n",
      "Sample of .1 regular Decision Tree {'Name': 'dt', 'Accuracy': 0.996268656716418, 'F1': 0.9962686393993232}\n",
      "Sample of .01 regular Decision Tree {'Name': 'dt', 'Accuracy': 0.9067164179104478, 'F1': 0.906711366716044}\n",
      "Sample of .1 regular Decision Tree {'Name': 'dt', 'Accuracy': 0.9987562189054726, 'F1': 0.9987564002752234}\n",
      "Sample of .01 regular Decision Tree {'Name': 'dt', 'Accuracy': 0.9328358208955224, 'F1': 0.9330652936746269}\n",
      "Sample of .1 regular Decision Tree {'Name': 'dt', 'Accuracy': 0.9950248756218906, 'F1': 0.9950239507572468}\n",
      "Sample of .01 regular Decision Tree {'Name': 'dt', 'Accuracy': 0.945273631840796, 'F1': 0.9452634583297147}\n",
      "Sample of .1 regular Decision Tree {'Name': 'dt', 'Accuracy': 0.9888059701492538, 'F1': 0.988803204718259}\n",
      "Sample of .01 regular Decision Tree {'Name': 'dt', 'Accuracy': 0.945273631840796, 'F1': 0.9453123237900289}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>name</th>\n",
       "      <th>sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.995033</td>\n",
       "      <td>0.995031</td>\n",
       "      <td>dtnorm</td>\n",
       "      <td>0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.996274</td>\n",
       "      <td>0.993789</td>\n",
       "      <td>mlpnorm</td>\n",
       "      <td>0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.930448</td>\n",
       "      <td>0.930435</td>\n",
       "      <td>gausnorm</td>\n",
       "      <td>0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.951588</td>\n",
       "      <td>0.951553</td>\n",
       "      <td>svcnorm</td>\n",
       "      <td>0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.692073</td>\n",
       "      <td>0.705590</td>\n",
       "      <td>gaus</td>\n",
       "      <td>0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>0.755543</td>\n",
       "      <td>0.756219</td>\n",
       "      <td>svcnorm</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>0.653255</td>\n",
       "      <td>0.675373</td>\n",
       "      <td>gaus</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>0.901541</td>\n",
       "      <td>0.901741</td>\n",
       "      <td>svc</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>0.658264</td>\n",
       "      <td>0.692786</td>\n",
       "      <td>mlp</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>0.963876</td>\n",
       "      <td>0.963930</td>\n",
       "      <td>dt</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           F1  accuracy      name  sample\n",
       "0    0.995033  0.995031    dtnorm   0.100\n",
       "1    0.996274  0.993789   mlpnorm   0.100\n",
       "2    0.930448  0.930435  gausnorm   0.100\n",
       "3    0.951588  0.951553   svcnorm   0.100\n",
       "4    0.692073  0.705590      gaus   0.100\n",
       "..        ...       ...       ...     ...\n",
       "315  0.755543  0.756219   svcnorm   0.005\n",
       "316  0.653255  0.675373      gaus   0.005\n",
       "317  0.901541  0.901741       svc   0.005\n",
       "318  0.658264  0.692786       mlp   0.005\n",
       "319  0.963876  0.963930        dt   0.005\n",
       "\n",
       "[320 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = pd.DataFrame()\n",
    "classifiers = ['gaus', 'svc', 'mlp', 'dt']\n",
    "k = KFold(n_splits=10)\n",
    "k.get_n_splits(X_train)\n",
    "for train_index, test_index in k.split(X_train):\n",
    "    X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]\n",
    "    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n",
    "    sample1 = enseSample(X_train_fold, .1)\n",
    "    sample1Fit = ensemble_fit(sample1,X_train_fold,y_train_fold)\n",
    "    sample1Pred = ensemble_predict(sample1Fit, X_test_fold)\n",
    "    print('Sample of .1 regular Decision Tree', ClfPipe(DecisionTreeClassifier(), X_train_fold[sample1[0][0]['indices']], y_train_fold[sample1[0][0]['indices']], X_test_fold, y_test_fold, 'dt').metrics())\n",
    "    score = score.append({'name': 'dtnorm', 'sample':.1, 'accuracy': ClfPipe(DecisionTreeClassifier(), X_train_fold[sample1[0][0]['indices']], y_train_fold[sample1[0][0]['indices']], X_test_fold, y_test_fold, 'dt').metrics()['Accuracy'], 'F1':ClfPipe(DecisionTreeClassifier(), X_train_fold[sample1[0][0]['indices']], y_train_fold[sample1[0][0]['indices']], X_test_fold, y_test_fold, 'dt').metrics()['F1'] }, ignore_index=True)\n",
    "    score = score.append({'name': 'mlpnorm', 'sample':.1, 'accuracy': ClfPipe(MLPClassifier(), X_train_fold[sample1[0][0]['indices']], y_train_fold[sample1[0][0]['indices']], X_test_fold, y_test_fold, 'dt').metrics()['Accuracy'], 'F1':ClfPipe(MLPClassifier(), X_train_fold[sample1[0][0]['indices']], y_train_fold[sample1[0][0]['indices']], X_test_fold, y_test_fold, 'dt').metrics()['F1'] }, ignore_index=True)\n",
    "    score = score.append({'name': 'gausnorm', 'sample':.1, 'accuracy': ClfPipe(GaussianNB(), X_train_fold[sample1[0][0]['indices']], y_train_fold[sample1[0][0]['indices']], X_test_fold, y_test_fold, 'dt').metrics()['Accuracy'], 'F1':ClfPipe(GaussianNB(), X_train_fold[sample1[0][0]['indices']], y_train_fold[sample1[0][0]['indices']], X_test_fold, y_test_fold, 'dt').metrics()['F1'] }, ignore_index=True)\n",
    "    score = score.append({'name': 'svcnorm', 'sample':.1, 'accuracy': ClfPipe(SVC(kernel='linear', probability=True), X_train_fold[sample1[0][0]['indices']], y_train_fold[sample1[0][0]['indices']], X_test_fold, y_test_fold, 'dt').metrics()['Accuracy'], 'F1':ClfPipe(SVC(kernel='linear', probability=True), X_train_fold[sample1[0][0]['indices']], y_train_fold[sample1[0][0]['indices']], X_test_fold, y_test_fold, 'dt').metrics()['F1'] }, ignore_index=True)\n",
    "    for index in range(len(sample1Pred)):\n",
    "        score =score.append({'name': classifiers[index], 'sample': .1, 'accuracy': accuracy_score(y_test_fold, sample1Pred[index]), \n",
    "                       'F1': f1_score(y_test_fold, sample1Pred[index], average='weighted')}, ignore_index=True)\n",
    "    sample01 = enseSample(X_train_fold, .01) #had to change to .01 sample. I was running into one problem where only one class would be sampled\n",
    "    sample01Fit = ensemble_fit(sample01,X_train_fold,y_train_fold)\n",
    "    sample01Pred = ensemble_predict(sample01Fit, X_test_fold)\n",
    "    print('Sample of .01 regular Decision Tree', ClfPipe(DecisionTreeClassifier(), X_train_fold[sample01[0][0]['indices']], y_train_fold[sample01[0][0]['indices']], X_test_fold, y_test_fold, 'dt').metrics())\n",
    "    score = score.append({'name': 'dtnorm', 'sample':.01, 'accuracy': ClfPipe(DecisionTreeClassifier(), X_train_fold[sample01[0][0]['indices']], y_train_fold[sample01[0][0]['indices']], X_test_fold, y_test_fold, 'dt').metrics()['Accuracy'], 'F1':ClfPipe(DecisionTreeClassifier(), X_train_fold[sample01[0][0]['indices']], y_train_fold[sample01[0][0]['indices']], X_test_fold, y_test_fold, 'dt').metrics()['F1'] }, ignore_index=True)\n",
    "    score = score.append({'name': 'mlpnorm', 'sample':.01, 'accuracy': ClfPipe(MLPClassifier(), X_train_fold[sample01[0][0]['indices']], y_train_fold[sample01[0][0]['indices']], X_test_fold, y_test_fold, 'dt').metrics()['Accuracy'], 'F1':ClfPipe(MLPClassifier(), X_train_fold[sample01[0][0]['indices']], y_train_fold[sample01[0][0]['indices']], X_test_fold, y_test_fold, 'dt').metrics()['F1'] }, ignore_index=True)\n",
    "    score = score.append({'name': 'gausnorm', 'sample':.01, 'accuracy': ClfPipe(GaussianNB(), X_train_fold[sample01[0][0]['indices']], y_train_fold[sample01[0][0]['indices']], X_test_fold, y_test_fold, 'dt').metrics()['Accuracy'], 'F1':ClfPipe(GaussianNB(), X_train_fold[sample01[0][0]['indices']], y_train_fold[sample01[0][0]['indices']], X_test_fold, y_test_fold, 'dt').metrics()['F1'] }, ignore_index=True)\n",
    "    score = score.append({'name': 'svcnorm', 'sample':.01, 'accuracy': ClfPipe(SVC(kernel='linear', probability=True), X_train_fold[sample01[0][0]['indices']], y_train_fold[sample01[0][0]['indices']], X_test_fold, y_test_fold, 'dt').metrics()['Accuracy'], 'F1':ClfPipe(SVC(kernel='linear', probability=True), X_train_fold[sample01[0][0]['indices']], y_train_fold[sample01[0][0]['indices']], X_test_fold, y_test_fold, 'dt').metrics()['F1'] }, ignore_index=True)\n",
    "    for index in range(len(sample01Pred)):\n",
    "        score =score.append({'name': classifiers[index], 'sample': .01, 'accuracy': accuracy_score(y_test_fold, sample01Pred[index]), \n",
    "                       'F1': f1_score(y_test_fold, sample01Pred[index], average='weighted')}, ignore_index=True)\n",
    "    sample05 = enseSample(X_train_fold, .05) #had to change to .01 sample. I was running into one problem where only one class would be sampled\n",
    "    sample05Fit = ensemble_fit(sample05,X_train_fold,y_train_fold)\n",
    "    sample05Pred = ensemble_predict(sample05Fit, X_test_fold)\n",
    "    score = score.append({'name': 'dtnorm', 'sample':.05, 'accuracy': ClfPipe(DecisionTreeClassifier(), X_train_fold[sample05[0][0]['indices']], y_train_fold[sample05[0][0]['indices']], X_test_fold, y_test_fold, 'dt').metrics()['Accuracy'], 'F1':ClfPipe(DecisionTreeClassifier(), X_train_fold[sample05[0][0]['indices']], y_train_fold[sample05[0][0]['indices']], X_test_fold, y_test_fold, 'dt').metrics()['F1'] }, ignore_index=True)\n",
    "    score = score.append({'name': 'mlpnorm', 'sample':.05, 'accuracy': ClfPipe(MLPClassifier(), X_train_fold[sample05[0][0]['indices']], y_train_fold[sample05[0][0]['indices']], X_test_fold, y_test_fold, 'dt').metrics()['Accuracy'], 'F1':ClfPipe(MLPClassifier(), X_train_fold[sample05[0][0]['indices']], y_train_fold[sample05[0][0]['indices']], X_test_fold, y_test_fold, 'dt').metrics()['F1'] }, ignore_index=True)\n",
    "    score = score.append({'name': 'gausnorm', 'sample':.05, 'accuracy': ClfPipe(GaussianNB(), X_train_fold[sample05[0][0]['indices']], y_train_fold[sample05[0][0]['indices']], X_test_fold, y_test_fold, 'dt').metrics()['Accuracy'], 'F1':ClfPipe(GaussianNB(), X_train_fold[sample05[0][0]['indices']], y_train_fold[sample05[0][0]['indices']], X_test_fold, y_test_fold, 'dt').metrics()['F1'] }, ignore_index=True)\n",
    "    score = score.append({'name': 'svcnorm', 'sample':.05, 'accuracy': ClfPipe(SVC(kernel='linear', probability=True), X_train_fold[sample05[0][0]['indices']], y_train_fold[sample05[0][0]['indices']], X_test_fold, y_test_fold, 'dt').metrics()['Accuracy'], 'F1':ClfPipe(SVC(kernel='linear', probability=True), X_train_fold[sample05[0][0]['indices']], y_train_fold[sample05[0][0]['indices']], X_test_fold, y_test_fold, 'dt').metrics()['F1'] }, ignore_index=True)\n",
    "    for index in range(len(sample05Pred)):\n",
    "        score =score.append({'name': classifiers[index], 'sample': .05, 'accuracy': accuracy_score(y_test_fold, sample05Pred[index]), \n",
    "                       'F1': f1_score(y_test_fold, sample05Pred[index], average='weighted')}, ignore_index=True)\n",
    "    sample005 = enseSample(X_train_fold, .005) #had to change to .01 sample. I was running into one problem where only one class would be sampled\n",
    "    sample005Fit = ensemble_fit(sample005,X_train_fold,y_train_fold)\n",
    "    sample005Pred = ensemble_predict(sample005Fit, X_test_fold)\n",
    "    score = score.append({'name': 'dtnorm', 'sample':.005, 'accuracy': ClfPipe(DecisionTreeClassifier(), X_train_fold[sample005[0][0]['indices']], y_train_fold[sample005[0][0]['indices']], X_test_fold, y_test_fold, 'dt').metrics()['Accuracy'], 'F1':ClfPipe(DecisionTreeClassifier(), X_train_fold[sample005[0][0]['indices']], y_train_fold[sample005[0][0]['indices']], X_test_fold, y_test_fold, 'dt').metrics()['F1'] }, ignore_index=True)\n",
    "    score = score.append({'name': 'mlpnorm', 'sample':.005, 'accuracy': ClfPipe(MLPClassifier(), X_train_fold[sample005[0][0]['indices']], y_train_fold[sample005[0][0]['indices']], X_test_fold, y_test_fold, 'dt').metrics()['Accuracy'], 'F1':ClfPipe(MLPClassifier(), X_train_fold[sample005[0][0]['indices']], y_train_fold[sample005[0][0]['indices']], X_test_fold, y_test_fold, 'dt').metrics()['F1'] }, ignore_index=True)\n",
    "    score = score.append({'name': 'gausnorm', 'sample':.005, 'accuracy': ClfPipe(GaussianNB(), X_train_fold[sample005[0][0]['indices']], y_train_fold[sample005[0][0]['indices']], X_test_fold, y_test_fold, 'dt').metrics()['Accuracy'], 'F1':ClfPipe(GaussianNB(), X_train_fold[sample005[0][0]['indices']], y_train_fold[sample005[0][0]['indices']], X_test_fold, y_test_fold, 'dt').metrics()['F1'] }, ignore_index=True)\n",
    "    score = score.append({'name': 'svcnorm', 'sample':.005, 'accuracy': ClfPipe(SVC(kernel='linear', probability=True), X_train_fold[sample005[0][0]['indices']], y_train_fold[sample005[0][0]['indices']], X_test_fold, y_test_fold, 'dt').metrics()['Accuracy'], 'F1':ClfPipe(SVC(kernel='linear', probability=True), X_train_fold[sample005[0][0]['indices']], y_train_fold[sample005[0][0]['indices']], X_test_fold, y_test_fold, 'dt').metrics()['F1'] }, ignore_index=True)\n",
    "    for index in range(len(sample005Pred)):\n",
    "        score =score.append({'name': classifiers[index], 'sample': .005, 'accuracy': accuracy_score(y_test_fold, sample005Pred[index]), \n",
    "                       'F1': f1_score(y_test_fold, sample005Pred[index], average='weighted')}, ignore_index=True)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66b3504e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>sample</th>\n",
       "      <th>F1</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dt</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.969391</td>\n",
       "      <td>0.969411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dt</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.984821</td>\n",
       "      <td>0.984829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dt</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.989550</td>\n",
       "      <td>0.989555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dt</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.990172</td>\n",
       "      <td>0.990176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dtnorm</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.899078</td>\n",
       "      <td>0.895180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dtnorm</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.937607</td>\n",
       "      <td>0.949388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dtnorm</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.983086</td>\n",
       "      <td>0.981350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dtnorm</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.994033</td>\n",
       "      <td>0.994032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gaus</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.672642</td>\n",
       "      <td>0.693482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gaus</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.622455</td>\n",
       "      <td>0.654440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gaus</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.664126</td>\n",
       "      <td>0.684652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gaus</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.763152</td>\n",
       "      <td>0.772939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>gausnorm</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.667111</td>\n",
       "      <td>0.691720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>gausnorm</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.642601</td>\n",
       "      <td>0.676219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>gausnorm</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.672054</td>\n",
       "      <td>0.696200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>gausnorm</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.799845</td>\n",
       "      <td>0.810964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mlp</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.447926</td>\n",
       "      <td>0.542034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>mlp</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.476122</td>\n",
       "      <td>0.580466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>mlp</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.587102</td>\n",
       "      <td>0.632214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>mlp</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.746660</td>\n",
       "      <td>0.758509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>mlpnorm</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.868035</td>\n",
       "      <td>0.865830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>mlpnorm</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.903766</td>\n",
       "      <td>0.902753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>mlpnorm</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.965788</td>\n",
       "      <td>0.963195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>mlpnorm</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.988806</td>\n",
       "      <td>0.989679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>svc</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.907715</td>\n",
       "      <td>0.907983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>svc</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.925487</td>\n",
       "      <td>0.925639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>svc</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.642481</td>\n",
       "      <td>0.660035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>svc</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.533195</td>\n",
       "      <td>0.548868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>svcnorm</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.814264</td>\n",
       "      <td>0.814970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>svcnorm</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.906730</td>\n",
       "      <td>0.906739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>svcnorm</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.948617</td>\n",
       "      <td>0.948644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>svcnorm</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.952239</td>\n",
       "      <td>0.952250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        name  sample        F1  accuracy\n",
       "0         dt   0.005  0.969391  0.969411\n",
       "1         dt   0.010  0.984821  0.984829\n",
       "2         dt   0.050  0.989550  0.989555\n",
       "3         dt   0.100  0.990172  0.990176\n",
       "4     dtnorm   0.005  0.899078  0.895180\n",
       "5     dtnorm   0.010  0.937607  0.949388\n",
       "6     dtnorm   0.050  0.983086  0.981350\n",
       "7     dtnorm   0.100  0.994033  0.994032\n",
       "8       gaus   0.005  0.672642  0.693482\n",
       "9       gaus   0.010  0.622455  0.654440\n",
       "10      gaus   0.050  0.664126  0.684652\n",
       "11      gaus   0.100  0.763152  0.772939\n",
       "12  gausnorm   0.005  0.667111  0.691720\n",
       "13  gausnorm   0.010  0.642601  0.676219\n",
       "14  gausnorm   0.050  0.672054  0.696200\n",
       "15  gausnorm   0.100  0.799845  0.810964\n",
       "16       mlp   0.005  0.447926  0.542034\n",
       "17       mlp   0.010  0.476122  0.580466\n",
       "18       mlp   0.050  0.587102  0.632214\n",
       "19       mlp   0.100  0.746660  0.758509\n",
       "20   mlpnorm   0.005  0.868035  0.865830\n",
       "21   mlpnorm   0.010  0.903766  0.902753\n",
       "22   mlpnorm   0.050  0.965788  0.963195\n",
       "23   mlpnorm   0.100  0.988806  0.989679\n",
       "24       svc   0.005  0.907715  0.907983\n",
       "25       svc   0.010  0.925487  0.925639\n",
       "26       svc   0.050  0.642481  0.660035\n",
       "27       svc   0.100  0.533195  0.548868\n",
       "28   svcnorm   0.005  0.814264  0.814970\n",
       "29   svcnorm   0.010  0.906730  0.906739\n",
       "30   svcnorm   0.050  0.948617  0.948644\n",
       "31   svcnorm   0.100  0.952239  0.952250"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = score.groupby(['name', 'sample']).mean().reset_index()\n",
    "grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d9f1e6",
   "metadata": {},
   "source": [
    "## 6. [20 pts] Report and plot 10-fold CV performances of the ensembles for the training subsample ratios of (0.0005, 0.001, 0.005, 0.01, 0.03, 0.05, 0.1) on the same graph. Add the regular classifiers to the plot with same subsample ratios. (Hint: pass the regular classifier to the same ensemble CV in a list of one element. Same script can be used for this entire step) Report your detailed observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b5c74b",
   "metadata": {},
   "source": [
    "The second part of this question really does not make sense, so i'm just doing single classifier on a subsample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a217cfae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='sample', ylabel='accuracy'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 842.4x595.44 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "g = sns.scatterplot(data=grouped, x='sample', y='accuracy', hue='name', s=100)\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf2174c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
